{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "757b4bc40ae9b28ccfad873662d95388",
     "grade": false,
     "grade_id": "cell-fc9b2593ea47645a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Social Media Analytics\n",
    "## School of Information, University of Michigan\n",
    "\n",
    "## Week 1: \n",
    " \n",
    "- Intro to social data: types of data, platforms, profiles\n",
    "- Ethical considerations in working with social media data\n",
    "- Intro to platform APIs: obtaining and managing data\n",
    "- Understanding the structure of social platform data\n",
    "\n",
    "\n",
    "## Assignment Overview\n",
    "### The objective of this assignment is to:\n",
    "\n",
    "- Access social platform data using an API\n",
    "    - figure out how to get and use authentication credentials\n",
    "- Manipulate the data accessed using python\n",
    "\n",
    "### The total score of this assignment will be 100 points consisting of:\n",
    "\n",
    "- retrieve one tweet: 20 points\n",
    "- retrieve one follower: 20 points\n",
    "- `create_tweet_df` function: 20 points\n",
    "- `create_hashtag_df` function: 20 points\n",
    "- `create_weekday_hour_count_df` function: 20 points\n",
    "\n",
    "### Resources:\n",
    "\n",
    " - [Tweepy API documentation (v4.10.0 is used for this assignment)](https://docs.tweepy.org/en/v4.10.0/index.html)\n",
    " - [Tweepy Getting Started Tutorial](https://docs.tweepy.org/en/v4.10.0/getting_started.html)\n",
    " - [Twitter API documentation](https://developer.twitter.com/en/docs/api-reference-index) \n",
    "\n",
    "### Instructions: \n",
    "In the first part of this assignment, you will use the Twitter API, Tweepy, and the Twitter API documentation to guide you through the process of obtaining social platform data. Once obtained, you will manipulate the data using Python to explore the types of data found on social platforms and the way in which those data are structured. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fb3e48b3d9aeb9f3dbf75fc29e5be86",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Part A (100 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f936def11c0de484dd9d675e09b7dc38",
     "grade": false,
     "grade_id": "cell-4879da2e3ba6125f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Important Note\n",
    "You can execute calls to Twitter's API in your notebook. The autograder **can not**.\n",
    "\n",
    "To get around this limitation, we ask you for the first two problems to *paste a text string that is extracted from the results of a call to the Twitter API*.\n",
    "\n",
    "Once you have run your code that produces the string that you need, you should comment out your code that produced it. **If you don't comment out your code that makes Twitter API calls, the auto-grader will fail and won't score the rest of your cells**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6165f324e4865a9ca35875e4cb51d258",
     "grade": true,
     "grade_id": "cell-c5589e1c99c131b2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "consumer_key = 'T8arnJa2yNOHdv12KaS2Pfw0o'\n",
    "consumer_secret = 'uaMO5GUL8XKmmqTBvaw6DVC2TwsZdb5kT1oWbbD5clVOTmgBEG'\n",
    "access_token = '792092174-tKgt38gLpmDD1moR6QsB9EhK7Bo4Rixa6tuFcwby'\n",
    "access_token_secret = 'tHtpM7EO4hRGM5lnyWqdvbQtyb1GqQvhMC83XdF8dxQAz'  \n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94da3d7ae62c029de5099c43634e3a0a",
     "grade": false,
     "grade_id": "cell-9790021ad923d3f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Get the data for one tweet as a json-formatted string\n",
    "20 points\n",
    "1. Call Twitter's API to get data for a single tweet\n",
    "    - One way is to make an API request for a single tweet. You can find one using the twitter website (check the URL string).\n",
    "    - Alternately, you can use an API call that returns data for multiple tweets. But be sure that you extract just a single one.\n",
    "2. Serialize the python object as a JSON formatted string, print it out, and copy the string literal\n",
    "    - Hint: json.dumps\n",
    "    \n",
    "3. Save it to a variable called `tweet_data_string`. Note that you will need to assign a hard-coded string literal to this variable, the string that you copied in the previous step.\n",
    "\n",
    "Note: the example provided shows only a few fields on the tweet object. Your solution should include all the fields that the \n",
    "          tweet json includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2f3bca863b91f1f8f9e2d80f6259d8c",
     "grade": false,
     "grade_id": "tweet_sample",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#one_tweet = api.user_timeline(screen_name = 'AdamSchefter', count = 1, include_rts = False)\n",
    "#one_tweet_json = one_tweet[0]._json\n",
    "#json.dumps(one_tweet_json)\n",
    "\n",
    "# You will need to replace the string literal below with somethign more complete\n",
    "tweet_data_string = '{\"created_at\": \"Thu Oct 27 21:37:25 +0000 2022\", \"id\": 1585747520705150976, \"id_str\": \"1585747520705150976\", \"text\": \"Cowboys added Micah Parsons and Sam Williams to their injury report today; Parsons was limited with a shoulder issu\\\\u2026 https://t.co/YSrfaLZyO7\", \"truncated\": true, \"entities\": {\"hashtags\": [], \"symbols\": [], \"user_mentions\": [], \"urls\": [{\"url\": \"https://t.co/YSrfaLZyO7\", \"expanded_url\": \"https://twitter.com/i/web/status/1585747520705150976\", \"display_url\": \"twitter.com/i/web/status/1\\\\u2026\", \"indices\": [117, 140]}]}, \"source\": \"<a href=\\\\\"http://twitter.com/download/iphone\\\\\" rel=\\\\\"nofollow\\\\\">Twitter for iPhone</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_status_id_str\": null, \"in_reply_to_user_id\": null, \"in_reply_to_user_id_str\": null, \"in_reply_to_screen_name\": null, \"user\": {\"id\": 51263592, \"id_str\": \"51263592\", \"name\": \"Adam Schefter\", \"screen_name\": \"AdamSchefter\", \"location\": \"New York\", \"description\": \"ESPN Senior NFL Insider. \\\\nInterview & Podcast Requests: ESPNPR@espn.com\\\\nHost of the Adam Schefter Podcast\\\\nhttps://t.co/oz43ix5jZU\", \"url\": \"https://t.co/OUz4GsIjV2\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"https://t.co/OUz4GsIjV2\", \"expanded_url\": \"https://open.spotify.com/show/42oVKqEPuf4kFZrXCO7Oh9?si=O1I6pHOYRh2FwJ1R0WbMSQ\", \"display_url\": \"open.spotify.com/show/42oVKqEPu\\\\u2026\", \"indices\": [0, 23]}]}, \"description\": {\"urls\": [{\"url\": \"https://t.co/oz43ix5jZU\", \"expanded_url\": \"http://Instagram.com/AdamSchefter\", \"display_url\": \"Instagram.com/AdamSchefter\", \"indices\": [106, 129]}]}}, \"protected\": false, \"followers_count\": 9910403, \"friends_count\": 3645, \"listed_count\": 53414, \"created_at\": \"Fri Jun 26 22:55:28 +0000 2009\", \"favourites_count\": 795, \"utc_offset\": null, \"time_zone\": null, \"geo_enabled\": true, \"verified\": true, \"statuses_count\": 62172, \"lang\": null, \"contributors_enabled\": false, \"is_translator\": false, \"is_translation_enabled\": true, \"profile_background_color\": \"2573B8\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme4/bg.gif\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme4/bg.gif\", \"profile_background_tile\": false, \"profile_image_url\": \"http://pbs.twimg.com/profile_images/793924061843914752/ycm8ibEE_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/793924061843914752/ycm8ibEE_normal.jpg\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/51263592/1466784770\", \"profile_link_color\": \"3B94D9\", \"profile_sidebar_border_color\": \"FFFFFF\", \"profile_sidebar_fill_color\": \"95E8EC\", \"profile_text_color\": \"16101F\", \"profile_use_background_image\": true, \"has_extended_profile\": true, \"default_profile\": false, \"default_profile_image\": false, \"following\": false, \"follow_request_sent\": false, \"notifications\": false, \"translator_type\": \"none\", \"withheld_in_countries\": []}, \"geo\": null, \"coordinates\": null, \"place\": null, \"contributors\": null, \"is_quote_status\": false, \"retweet_count\": 57, \"favorite_count\": 480, \"favorited\": false, \"retweeted\": false, \"lang\": \"en\"}'\n",
    "json_string = tweet_data_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f25e8e3d9ae285916dbf9e634753562b",
     "grade": true,
     "grade_id": "tweet_sample_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Some tests for your code in the previous cell. \n",
    "# There are additional hidden tests not shown in this cell that the autograder uses.\n",
    "import json\n",
    "from jsonschema import validate, ValidationError\n",
    "schema = '{\"definitions\": {}, \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"$id\": \"http://example.com/root.json\", \"type\": \"object\", \"additionalProperties\": true, \"title\": \"The Root Schema\", \"required\": [\"created_at\", \"id\", \"id_str\"], \"properties\": {\"created_at\": {\"$id\": \"#/properties/created_at\", \"type\": \"string\", \"title\": \"The Created_at Schema\", \"default\": \"\", \"examples\": [\"Wed Nov 20 14:14:20 +0000 2019\"], \"pattern\": \"^(.*)$\"}, \"id\": {\"$id\": \"#/properties/id\", \"type\": \"integer\", \"title\": \"The Id Schema\", \"default\": 0, \"examples\": [1197156225588301800]}, \"id_str\": {\"$id\": \"#/properties/id_str\", \"type\": \"string\", \"title\": \"The Id_str Schema\", \"default\": \"\", \"examples\": [\"1197156225588301824\"], \"pattern\": \"^(.*)$\"}}}'\n",
    "validation_schema = json.loads(schema)\n",
    "sample = json.loads(tweet_data_string)\n",
    "raised = False\n",
    "try:\n",
    "    validate(sample, schema=validation_schema)\n",
    "except:\n",
    "    raised = True\n",
    "assert raised == False, 'tweet_data_string, Json schema is not correct'\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e075d38ba86639aaec68d42fe8568b8b",
     "grade": false,
     "grade_id": "cell-558ca9d0397196cf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Get the data for a single follower of the umsi twitter account\n",
    "\n",
    "20 points.\n",
    "\n",
    "Again, dump it out as a json formatted string, print it, and paste it as a string literal assigning it to the variable `follower_data_as_string`. Don't forget to comment out your code that makes Twitter API calls.\n",
    "\n",
    "Note: you should paste the data for just a single follower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec1dd2b1b002859f3ff43f62ce9525aa",
     "grade": false,
     "grade_id": "follower_sample",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#one_follower = api.get_followers(screen_name = 'umsi', count = 1)\n",
    "#one_follower_json = one_follower[0]._json\n",
    "#json.dumps(one_follower_json)\n",
    "\n",
    "# replace the string literal below\n",
    "follower_data_as_string = '{\"id\": 1225022693189341184, \"id_str\": \"1225022693189341184\", \"name\": \"Christian Caballero\", \"screen_name\": \"chriscaballero_\", \"location\": \"He/Him\", \"description\": \"polisci phd student @UCberkeley | previously @NYUniversity | RBSI 21 | RTs = part of the presentation of self\", \"url\": null, \"entities\": {\"description\": {\"urls\": []}}, \"protected\": false, \"followers_count\": 163, \"friends_count\": 685, \"listed_count\": 0, \"created_at\": \"Wed Feb 05 11:45:57 +0000 2020\", \"favourites_count\": 666, \"utc_offset\": null, \"time_zone\": null, \"geo_enabled\": false, \"verified\": false, \"statuses_count\": 59, \"lang\": null, \"status\": {\"created_at\": \"Wed Jul 13 23:36:13 +0000 2022\", \"id\": 1547364305175289857, \"id_str\": \"1547364305175289857\", \"text\": \"RT @DavMicRot: \\\\ud83d\\\\udea8New Paper\\\\ud83d\\\\udea8\\\\\"Quantifying Partisan News Diets in Web and TV Audiences\\\\\" in @ScienceAdvances with Dan Muise @homahmrd Baird Howl\\\\u2026\", \"truncated\": false, \"entities\": {\"hashtags\": [], \"symbols\": [], \"user_mentions\": [{\"screen_name\": \"DavMicRot\", \"name\": \"David Rothschild \\\\ud83c\\\\udf3b\", \"id\": 108977933, \"id_str\": \"108977933\", \"indices\": [3, 13]}, {\"screen_name\": \"ScienceAdvances\", \"name\": \"Science Advances\", \"id\": 2326120952, \"id_str\": \"2326120952\", \"indices\": [87, 103]}, {\"screen_name\": \"homahmrd\", \"name\": \"Homa\", \"id\": 1125449088839774208, \"id_str\": \"1125449088839774208\", \"indices\": [119, 128]}], \"urls\": []}, \"source\": \"<a href=\\\\\"http://twitter.com/download/iphone\\\\\" rel=\\\\\"nofollow\\\\\">Twitter for iPhone</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_status_id_str\": null, \"in_reply_to_user_id\": null, \"in_reply_to_user_id_str\": null, \"in_reply_to_screen_name\": null, \"geo\": null, \"coordinates\": null, \"place\": null, \"contributors\": null, \"retweeted_status\": {\"created_at\": \"Wed Jul 13 18:59:17 +0000 2022\", \"id\": 1547294611621777410, \"id_str\": \"1547294611621777410\", \"text\": \"\\\\ud83d\\\\udea8New Paper\\\\ud83d\\\\udea8\\\\\"Quantifying Partisan News Diets in Web and TV Audiences\\\\\" in @ScienceAdvances with Dan Muise @homahmrd B\\\\u2026 https://t.co/RrtklyK0F8\", \"truncated\": true, \"entities\": {\"hashtags\": [], \"symbols\": [], \"user_mentions\": [{\"screen_name\": \"ScienceAdvances\", \"name\": \"Science Advances\", \"id\": 2326120952, \"id_str\": \"2326120952\", \"indices\": [72, 88]}, {\"screen_name\": \"homahmrd\", \"name\": \"Homa\", \"id\": 1125449088839774208, \"id_str\": \"1125449088839774208\", \"indices\": [104, 113]}], \"urls\": [{\"url\": \"https://t.co/RrtklyK0F8\", \"expanded_url\": \"https://twitter.com/i/web/status/1547294611621777410\", \"display_url\": \"twitter.com/i/web/status/1\\\\u2026\", \"indices\": [117, 140]}]}, \"source\": \"<a href=\\\\\"https://mobile.twitter.com\\\\\" rel=\\\\\"nofollow\\\\\">Twitter Web App</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_status_id_str\": null, \"in_reply_to_user_id\": null, \"in_reply_to_user_id_str\": null, \"in_reply_to_screen_name\": null, \"geo\": null, \"coordinates\": null, \"place\": null, \"contributors\": null, \"is_quote_status\": false, \"retweet_count\": 129, \"favorite_count\": 335, \"favorited\": false, \"retweeted\": false, \"possibly_sensitive\": false, \"lang\": \"en\"}, \"is_quote_status\": false, \"retweet_count\": 129, \"favorite_count\": 0, \"favorited\": false, \"retweeted\": false, \"lang\": \"en\"}, \"contributors_enabled\": false, \"is_translator\": false, \"is_translation_enabled\": false, \"profile_background_color\": \"F5F8FA\", \"profile_background_image_url\": null, \"profile_background_image_url_https\": null, \"profile_background_tile\": false, \"profile_image_url\": \"http://pbs.twimg.com/profile_images/1547020980874264576/opLb_ekm_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/1547020980874264576/opLb_ekm_normal.jpg\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/1225022693189341184/1580903877\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"has_extended_profile\": false, \"default_profile\": true, \"default_profile_image\": false, \"following\": false, \"live_following\": false, \"follow_request_sent\": false, \"notifications\": false, \"muting\": false, \"blocking\": false, \"blocked_by\": false, \"translator_type\": \"none\", \"withheld_in_countries\": []}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b18a18f43d381468cfaed8ba23c3219",
     "grade": true,
     "grade_id": "follower_sample_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Some tests for your code in the previous cell. \n",
    "# There are additional hidden tests not shown in this cell that the autograder uses.\n",
    "submission = follower_data_as_string\n",
    "assert type(submission) == str, 'follower_sample, function does not return a json string.'\n",
    "\n",
    "converted = json.loads(submission)\n",
    "assert type(converted) == dict, 'follower_sample, json string does not contain a dictionary response.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab591925982c711255bf91ca31bca37b",
     "grade": false,
     "grade_id": "cell-402a3615a2afe448",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Using Cached Data\n",
    "The remaining questions will have you process data of the kind that comes back from Twitter.\n",
    "\n",
    "So that you are working with the same data that our auto-grader is, you will be processing data that we have already retrieved and saved in a text file.\n",
    "\n",
    "In other words, congratulations on establishing that you can use the Twitter API to fetch data. But from here on out, you won't be using it for the graded assignments; you'll only be using data that we've already fetched and saved in files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd8f87299defc9d4d0ff255b2a89d890",
     "grade": false,
     "grade_id": "create_tweet_df",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def create_tweet_df(json_file_path):\n",
    "    \"\"\" 20 points\n",
    "        Transform the tweets_json object into a dataframe with the following columns and dataypes:\n",
    "        'retweet_count', int64\n",
    "        'created_at', datetime64[ns, UTC]\n",
    "        'full_text', object\n",
    "        'favorited', bool\n",
    "        'retweeted', bool\n",
    "        'lang', object\n",
    "        'favorite_count', int64\n",
    "        \n",
    "        Return the dataframe\n",
    "    \"\"\" \n",
    "    with open(json_file_path) as json_file:\n",
    "        tweets = json.load(json_file)\n",
    "    \n",
    "    df_list = []\n",
    "    for tweet in tweets:\n",
    "        tweet_list = [tweet['retweet_count'], tweet['created_at'], tweet['full_text'], tweet['favorited'],\n",
    "                      tweet['retweeted'], tweet['lang'], tweet['favorite_count']]\n",
    "        df_list.append(tweet_list)\n",
    "    \n",
    "    tweets_df = pd.DataFrame(df_list, columns = ['retweet_count', 'created_at', 'full_text', 'favorited',\n",
    "                                                 'retweeted', 'lang', 'favorite_count']) \n",
    "    \n",
    "    tweets_df['created_at'] =  pd.to_datetime(tweets_df['created_at'])\n",
    "\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf1a0fecf3285be6bf87beb9c5cb0acf",
     "grade": true,
     "grade_id": "create_tweet_df_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Some tests for your code in the previous cell. \n",
    "# There are additional hidden tests not shown in this cell that the autograder uses.\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = create_tweet_df('assets/POTUS_2019-03-07_2020-01-28.json')\n",
    "df_length = 3201\n",
    "assert len(df) == df_length, \"create_tweet_df, the length of the dataframe should be %d\" % df_length\n",
    "df_cols = ['retweet_count','created_at','full_text','favorited','retweeted','lang','favorite_count','retweet_count']\n",
    "for col_name in df_cols:\n",
    "    assert col_name in df.columns.values, \"create_tweet_df, the column %s should be included\" % col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b99ac18785d2502f6f816f43f3213e5",
     "grade": false,
     "grade_id": "create_hashtag_df",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def create_hashtag_df(json_file_path):\n",
    "    \"\"\" 20 points\n",
    "        Transform the tweets_json object into a dataframe with the following columns:\n",
    "        'text', object, the text of the hashtag\n",
    "        'user', object, the screen name of the user who tweeted; if it's a retweet then the retweeter, not the original tweeter\n",
    "        'created_at', datetime, the time the hashtag was tweeted\n",
    "        HINT: Use the entities.hashtags attribute in the tweet to build this dataframe\n",
    "    \"\"\" \n",
    "    with open(json_file_path) as json_file:\n",
    "        tweets = json.load(json_file)\n",
    "        \n",
    "    df_list = []\n",
    "    for tweet in tweets:\n",
    "        if len(tweet['entities']['hashtags']) > 0:\n",
    "            for i in range(len(tweet['entities']['hashtags'])):\n",
    "                tweet_list = [tweet['entities']['hashtags'][i]['text'], tweet['user']['screen_name'], tweet['created_at']]\n",
    "                df_list.append(tweet_list)\n",
    "    \n",
    "    hashtag_df = pd.DataFrame(df_list, columns = ['text', 'user', 'created_at']) \n",
    "    hashtag_df['created_at'] =  pd.to_datetime(hashtag_df['created_at'])\n",
    "\n",
    "    return hashtag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "129ccc51e9396c12eac974a00ad7d387",
     "grade": true,
     "grade_id": "create_hashtag_df_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Some tests for your code in the previous cell. \n",
    "# There are additional hidden tests not shown in this cell that the autograder uses.\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = create_hashtag_df('assets/POTUS_2019-03-07_2020-01-28.json')\n",
    "df_length = 86\n",
    "assert len(df) == df_length, \"create_hashtag_df, the length of the dataframe should be %d\" % df_length\n",
    "df_cols = ['text','user','created_at']\n",
    "for col_name in df_cols:\n",
    "    assert col_name in df.columns.values, \"create_hashtag_df, the column %s should be included\" % col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb58257d5795db6c7f3a7d91645efac9",
     "grade": false,
     "grade_id": "create_weekday_hour_count_df",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def create_weekday_hour_count_df(tweets_dataframe):\n",
    "    \"\"\" 20 points\n",
    "        Create a pivot table where the columns are the day hours (0 -23) \n",
    "        and rows are weekdays ('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday').\n",
    "        Each cell would be the count of tweets at a given weekday and a given hour.\n",
    "        If there are no values at a specific weekday and hour the value should be 0\n",
    "        Sort the hours in ascending order starting from 0 and the weekdays starting from Monday\n",
    "    \"\"\"\n",
    "\n",
    "    tweets_dataframe['day_of_week'] = tweets_dataframe['created_at'].dt.day_name()\n",
    "    tweets_dataframe['hour'] = tweets_dataframe['created_at'].dt.hour\n",
    "    tweets_dataframe['sum_by'] = 1\n",
    "    \n",
    "    missing_hours = [{'hour': 6, 'sum_by': 0, 'day_of_week': 'Monday'}, {'hour': 7, 'sum_by': 0, 'day_of_week': 'Monday'}]\n",
    "    appended_df = tweets_dataframe.append(missing_hours, ignore_index = True)\n",
    "\n",
    "    pivot_temp = pd.pivot_table(appended_df, index = 'day_of_week', columns = 'hour', values = 'sum_by', \n",
    "                                aggfunc = 'sum', fill_value = 0)\n",
    "\n",
    "    return pivot_temp.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ad4364390371dd58f308febe14c9909",
     "grade": true,
     "grade_id": "cell-create_weekday_hour_count_df_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Some tests for your code in the previous cell. \n",
    "# There are additional hidden tests not shown in this cell that the autograder uses.\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "t_df = create_tweet_df('assets/POTUS_2019-03-07_2020-01-28.json')\n",
    "df = create_weekday_hour_count_df(t_df)\n",
    "for col_name in range(0,24):\n",
    "    assert col_name in df.columns.values, \"create_weekday_hour_count_df, the column %s should be included\" % col_name\n",
    "for row_name in ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday','Wednesday']:\n",
    "    assert row_name in df.index.values, \"create_weekday_hour_count_df, the column %s should be included\" % row_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_social_media_analytics_v2_assignment1_part1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
