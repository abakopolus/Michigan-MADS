{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d1ea724a917fd50c388bf82488c2585",
     "grade": false,
     "grade_id": "cell-7885ab5957ec1d3a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"v1.12.052622.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7902f22ab018e6b89bba741482ad4fc3",
     "grade": false,
     "grade_id": "cell-64b4374420d1d161",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SIADS 543 Assignment 3: Text representations, topic modeling and word embeddings\n",
    "\n",
    "In this week's assignment you'll gain experience applying topic modeling and other latent variable estimation methods. We'll focus on textual data, continuing to work with vectorizers and related text representations like embeddings.\n",
    "\n",
    "All questions in this assignment are auto-graded. Some parts ask you a short question or two about on the results: these are meant to encourage you to reflect on the outcomes, but do not need to be included as part of your graded submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16476b741e0fe7ee24adbb4e5f3c4254",
     "grade": false,
     "grade_id": "cell-e8d8014231a6151c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# First import some necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fd1fedd0b2506667c9ccaf3417fe37a",
     "grade": false,
     "grade_id": "cell-8bf7ebbd04db03d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Here are some useful utility functions to use with this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efd5c2a82c614ee2aae9c53593aa72cc",
     "grade": false,
     "grade_id": "cell-172dd4cf129d7dcd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# display_topics:  example showing how to take the model components generated by LDA or NMF\n",
    "# and use them to dump the top words by weight for each topic.\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\n",
    "            \" \".join(\n",
    "                [feature_names[i] for i in topic.argsort()[: -num_top_words - 1 : -1]]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# load_newsgroup_documents: prepare training and test data from the 20newsgroups dataset\n",
    "def load_newsgroup_documents():\n",
    "    # The Coursera environment must be self-contained and so APIs that do external fetching\n",
    "    # aren't allowed. So we use pickle files that can be stored locally instead of the following\n",
    "    # API calls.\n",
    "    # dataset_train   = fetch_20newsgroups(subset = 'train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "    # dataset_test    = fetch_20newsgroups(subset = 'test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    pickle_train_data = open(\"./assets/20newsgroups_train_data.pickle\", \"rb\")\n",
    "    pickle_train_labels = open(\"./assets/20newsgroups_train_labels.pickle\", \"rb\")\n",
    "    documents_train = pickle.load(pickle_train_data)\n",
    "    labels_train = pickle.load(pickle_train_labels)\n",
    "    pickle_train_data.close()\n",
    "    pickle_train_labels.close()\n",
    "\n",
    "    return documents_train, labels_train\n",
    "\n",
    "\n",
    "# load the dataset for future use....\n",
    "documents_train, labels_train = load_newsgroup_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4d79eeb2c5bef0e04855f12c92664ac",
     "grade": false,
     "grade_id": "cell-8ceaa45190716940",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1 (20 points)  The choice of text processing can impact final classification performance.\n",
    "\n",
    "There are many different parameter settings for Vectorizer objects in scikit-learn. Small changes in these settings can result in very different text representations and significant changes in final classifier accuracy. For this question you'll train a commonly-used type of text classifier, Multinomial Naive Bayes, using three different input representations for text, to see the effect of different parameter choices on classifier training set accuracy.\n",
    "\n",
    "Follow these steps:\n",
    "1. Create a TfidfVectorizer object (let's call it A) with the following settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, # keep capitalization`\n",
    "    \n",
    "    `ngram_range = (1,2), # include 2-word phrases`\n",
    "    \n",
    "    `min_df=10,  # note: absolute count of documents`\n",
    "    \n",
    "    `max_df=0.95,   # note: % of docs in collection`\n",
    "    \n",
    "    `stop_words='english'`\n",
    "    \n",
    "    \n",
    "2. Create a CountVectorizer object (let's call it B) with the same settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, # keep capitalization`\n",
    "    \n",
    "    `ngram_range = (1,2), # include 2-word phrases`\n",
    "    \n",
    "    `min_df=10,  # note: absolute count of doc`\n",
    "    \n",
    "    `max_df=0.95,   # note: % of docs`\n",
    "    \n",
    "    `stop_words='english'`\n",
    "    \n",
    "3. Create a TfidfVectorizer object (let's call it C) with the settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, `\n",
    "    \n",
    "    `ngram_range = (1,2), `\n",
    "    \n",
    "    `min_df=200,  # note: absolute count of docs`\n",
    "    \n",
    "    `max_df=0.95  # note: % of docs` \n",
    "    \n",
    "    \n",
    "4. Using the training data `documents_train`, along with the ground truth labels `labels_train`, train three Naive Bayes classifiers, corresponding to choices A, B, and C of vectorizer.\n",
    "\n",
    "5. Normally we'd compute the accuracy of these classifiers on a test set, but for this question we're interested more in the potential upper bound on performance that is achievable with text representation choices A, B, or C.  Thus you should compute, for each of the three classifiers, the accuracy on the *training set*.\n",
    "\n",
    "6. Your function should return these three accuracy scores as a tuple with three floats: (accuracy_A, accuracy_B, accuracy_C).\n",
    "\n",
    "It is instructive to examine the difference in accuracy across the three different representations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b37bfd40e8b4cfdf1529d92e1b8ce08f",
     "grade": false,
     "grade_id": "cell-ce0aaf11d3f59591",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "def answer_text_processing():\n",
    "    \n",
    "    A = TfidfVectorizer(max_features = 10000, # only top 10k by freq\n",
    "                        lowercase = False, # keep capitalization\n",
    "                        ngram_range = (1,2), # include 2-word phrases\n",
    "                        min_df=10,  # note: absolute count of documents\n",
    "                        max_df=0.95,   # note: % of docs in collection\n",
    "                        stop_words='english') # default English stopwords\n",
    "\n",
    "    B = CountVectorizer(max_features = 10000, # only top 10k by freq\n",
    "                        lowercase = False, # keep capitalization\n",
    "                        ngram_range = (1,2), # include 2-word phrases\n",
    "                        min_df=10,  # note: absolute count of doc\n",
    "                        max_df=0.95,   # note: % of docs\n",
    "                        stop_words='english')\n",
    "\n",
    "    C = TfidfVectorizer(max_features = 10000, # only top 10k by freq\n",
    "                        lowercase = False,\n",
    "                        ngram_range = (1,2),\n",
    "                        min_df=200,  # note: absolute count of docs\n",
    "                        max_df=0.95)  # note: % of docs # default English stopwords\n",
    "    \n",
    "    naive_bayes_classifier = MultinomialNB()\n",
    "\n",
    "    A_documents = A.fit_transform(documents_train)\n",
    "    A_fit = naive_bayes_classifier.fit(A_documents, labels_train)\n",
    "    A_pred = A_fit.predict(A_documents)\n",
    "    A_score = metrics.accuracy_score(labels_train, A_pred)\n",
    "\n",
    "    B_documents = B.fit_transform(documents_train)\n",
    "    B_fit = naive_bayes_classifier.fit(B_documents, labels_train)\n",
    "    B_pred = B_fit.predict(B_documents)\n",
    "    B_score = metrics.accuracy_score(labels_train, B_pred)\n",
    "\n",
    "    C_documents = C.fit_transform(documents_train)\n",
    "    C_fit = naive_bayes_classifier.fit(C_documents, labels_train)\n",
    "    C_pred = C_fit.predict(C_documents)\n",
    "    C_score = metrics.accuracy_score(labels_train, C_pred)\n",
    "\n",
    "    result = A_score, B_score, C_score\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b32bab450e6fd00b3103102dc85620af",
     "grade": true,
     "grade_id": "cell-01b250516653cdd8",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_text_processing()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q1: Your function should return a tuple.\"\n",
    "assert len(stu_ans) == 3, \"Q1: Your tuple should contain three floats.\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Q1: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a55aa0ffb48e9a8a3fe9925e329136b3",
     "grade": false,
     "grade_id": "cell-3d0505dd6f24836c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2 (30 points). Latent Semantic Indexing and the vocabulary gap.\n",
    "\n",
    "One of the original motivations for Latent Semantic Indexing was overcoming the `vocabulary gap` in information retrieval.\n",
    "A query like `economic budget` should match strongly against text like `government spending on the economy` even though they don't have any exact keywords in common.\n",
    "\n",
    "In this question we'll create a demonstration of the power of Latent Semantic Indexing to do semantic matching. In the first part, you'll run LSI and use the reduced document matrix to do semantic matching of a query against other text that has no terms explicitly in common.\n",
    "\n",
    "In the second part, you'll see how this semantic matching is happening by computing the related terms that are included a query expanded using LSI's latent topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef907586d8ec6b72c4f4ffe58359edc7",
     "grade": false,
     "grade_id": "cell-0d79b1fd48df5fe5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Part 2.1 (15 points) Use the reduced document matrix from LSI to do semantic matching of a query against a document.\n",
    "\n",
    "As a first step, run the code below that we've provided that creates a tf.idf vectorizer and applies it to the 20newsgroups training set. It also runs LSI (in reality a TruncatedSVD) with a latent space of 200 dimensions.\n",
    "\n",
    "Suppose we have a query \"economic budget\" that has the tf.idf vector $q$, with shape 1 x num_terms. We can obtain this vector simply by using vectorizer.transform on the text. Think of the matrix $U_k$ as the super operator that converts from original term space to latent semantic space. To expand text $q$ with related terms according to LSI, compute the expanded query $q_k$ using the formula \n",
    "\n",
    "$q_k = q \\cdot (U_k\\Sigma^{-1}_k)$\n",
    "\n",
    "With this formula, you'll \"expand\" both the query and the document vectors to add related terms, and then compute the similarity match between them.\n",
    "\n",
    "Let's walk through these steps (**Note** that in matrix math dimensions and hence order of operations matters!).  \n",
    "1. The _reduced term matrix_, $U_k$, is _multiplied_ (* operator) with the inverse of matrix $\\Sigma_k$, the LSI.singular_values_ matrix. **Note** that $\\Sigma_k$ is raised to the power of negative one $\\Sigma^{-1}_k$. The reason we invert $\\Sigma_k$ is that there is no division operation with matrices so we invert and multiply!\n",
    " - Think of this step as forming a normalized scaler for the LSI latent factor weights (the 'topics').  \n",
    "\n",
    "\n",
    "2. The vectorized query matrix $q$ (or document $d$) is then dotted (dot-product) with the result of $U_k \\Sigma^{-1}_k$.\n",
    "\n",
    "For this question, use cosine similarity to compute the similarity match between any two pieces of text, no matter what their vector representation.\n",
    "\n",
    "With the formula above, consider the query `\"economic budget\"` being matched against the (very) short document `\"government spending on the economy\"`.\n",
    "\n",
    "Your function should return a tuple of two floats: the cosine similarity score (from sklearn.metrics.pairwise) of (a) the original query and document vectors and (b) the LSI-expanded query and document vectors using the method above.\n",
    "\n",
    "Did LSI help overcome the vocabulary gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64d7fdd39880946a41ec479609961cee",
     "grade": false,
     "grade_id": "cell-f7fad0f295241fbc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "### Run this preamble code to run LSI. We've also given the line of code that gets the resulting U matrix.\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 1), min_df=2, max_df=0.95, stop_words=\"english\", max_features=10000\n",
    ")  # default English stopwords\n",
    "\n",
    "tfidf_documents = tfidf_vectorizer.fit_transform(documents_train)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LSI does truncated SVD on the document-term matrix of tf.idf term-weights.\n",
    "# The matrix we got back from the vectorizer is a\n",
    "# document-term matrix, i.e. one row per document.\n",
    "n_topics = 200\n",
    "lsi = TruncatedSVD(n_components=n_topics, random_state=0)\n",
    "\n",
    "# To match the examples and development of LSI in\n",
    "# our lectures, we're going to\n",
    "# take the transpose of the document-term matrix to give\n",
    "# TruncatedSVD the term-document matrix as input.\n",
    "\n",
    "# This is the matrix U_k:  num_term_features x num_topics\n",
    "reduced_term_matrix = lsi.fit_transform(np.transpose(tfidf_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32c916f5026c1184987262097effb7e8",
     "grade": false,
     "grade_id": "cell-03e5c2867559c41b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def answer_semantic_similarity_a():\n",
    "    \n",
    "    q = tfidf_vectorizer.transform(['economic budget'])\n",
    "    d = tfidf_vectorizer.transform(['government spending on the economy'])\n",
    "    first_step = cosine_similarity(q,d).item(0) #0!\n",
    "\n",
    "    Uk_Sigma_q = reduced_term_matrix * (1/lsi.singular_values_)\n",
    "    new_q = np.dot(q.toarray(), Uk_Sigma_q)\n",
    "\n",
    "    Uk_Sigma_d = reduced_term_matrix * (1/lsi.singular_values_)\n",
    "    new_d = np.dot(d.toarray(), Uk_Sigma_d)\n",
    "    \n",
    "    second_step = cosine_similarity(new_q, new_d).item(0)\n",
    "\n",
    "    result = first_step, second_step\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0bf77e59ceb33b1ce472e31afeb0d7e",
     "grade": true,
     "grade_id": "cell-5b880b5a4ac457e5",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_semantic_similarity_a()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q2a: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Q2a: Your tuple should contain two floats.\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Q2a: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb9927364c943ea8c419a6e627e8cff9",
     "grade": false,
     "grade_id": "cell-7dcb7722fb75a664",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Part 2.2 (15 points): We want to understand this semantic matching ability a bit more: what terms does LSI think are similar?\n",
    "\n",
    "To understand why the LSI-expanded vectors get the results they do, we're going to look at what the operator $U$ does to text. In particular, the term-term matrix $UU^T$ tells us the term expansion behavior of this LSI model. Think of the term-term matrix like an operator that first maps a term to the latent space L_k (using $U$), then back again from L_k to term space (using $U$ transpose). The $(i,j)$ entry of $UU^T$ is a kind of *association weight* between term $i$ and term $j$.\n",
    "\n",
    "Write a function to get the most related terms (according to LSI) for the word \"economy\". To do this:\n",
    "\n",
    "1. Compute the term-term matrix from the matrix U  (the reduced_term_matrix variable).\n",
    "2. Use the term-term matrix to get the association weights of all words related to the term \"economy\"\n",
    "3. Sort by descending weight value.\n",
    "4. Your function should return the top 5 words and their weights as a list of (string, float) tuples.\n",
    "\n",
    "Do the related terms match your subjective similarity judgment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6fc0b432195a6641d5e16714e4ef324",
     "grade": false,
     "grade_id": "cell-980257c3a923230b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_semantic_similarity_b():\n",
    "\n",
    "    term_term_matrix = np.dot(reduced_term_matrix, np.transpose(reduced_term_matrix))\n",
    "    economy_index = tfidf_vectorizer.vocabulary_['economy']\n",
    "    \n",
    "    top_related_term_indexes = term_term_matrix[economy_index, :].argsort()[::-1]\n",
    "    result = []\n",
    "\n",
    "    for i in range(5):\n",
    "        next_term = top_related_term_indexes[i]\n",
    "        next_tuple = tfidf_feature_names[next_term], term_term_matrix[economy_index, next_term]\n",
    "        result.append(next_tuple)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd0f7c8c5dfdc80549da424080af3c4b",
     "grade": true,
     "grade_id": "cell-683419d1db09c762",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_semantic_similarity_b()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q2b: Your function should return a list. \"\n",
    "assert (\n",
    "    len(stu_ans) == 5\n",
    "), \"Q2b: Your list should contain five elements (the term, score tuples).\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(item, tuple), f\"Q2b: Your answer at index {i} should be a tuple. \"\n",
    "    assert isinstance(\n",
    "        item[0], str\n",
    "    ), f\"Q2b: The first element of your tuple at index {i} should be a string. \"\n",
    "    assert isinstance(\n",
    "        item[1], (float, np.floating)\n",
    "    ), f\"Q2b: The second element of your tuple at index {i} should be a float. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c962254a932754ed62505dcf56265be",
     "grade": false,
     "grade_id": "cell-e5ef4ed71becdbc8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3 (20 points) Semantic similarity: comparing your ranking with word2vec's ranking\n",
    "\n",
    "\n",
    "### Part 3.1 Ranking words by yourself and then generating word2vec's ranking\n",
    "\n",
    "First, rearrange the words in the `my_ranking` tuple based on your subjective impression of how similar they are to the word `\"party\"`. Encode your impression with words arranged in decreasing similarity to `\"party\"`; index 0 holds `\"party\"`, index 1 the next most similar term all the way to index 8 holding what you consider the least similar. **This is your personal subjective understanding of how similar these terms are to `\"party\"`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THE FOLLOWING variable my_ranking.\n",
    "#\n",
    "# The target word is **'party'**, which you should keep first in the tuple. Edit the order of the rest of the words\n",
    "# in my_ranking so that it reflects\n",
    "# YOUR subjective ranking for how semantically similar each word is to the word 'party'. For example,\n",
    "# if you think 'event' is the most similar word to 'party', it should be placed second in the list after 'party'\n",
    "# and so on. Make sure you use all the words : just re-order them.\n",
    "my_ranking = (\n",
    "    \"party\",\n",
    "    \"event\",\n",
    "    \"fun\",\n",
    "    \"champagne\",\n",
    "    \"budget\",\n",
    "    \"bicycle\",\n",
    "    \"vote\",\n",
    "    \"lead\",\n",
    "    \"election\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccb21b907655bfc87e045b68559e6058",
     "grade": false,
     "grade_id": "cell-17f6968583f94f48",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Before proceeding:\n",
    " - The following code cell must be executed to load the pre-trained word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5403b56274bae13b42285323ae86329",
     "grade": false,
     "grade_id": "cell-907764ada5cdd9f5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#### We need to load the pre-trained word2vec model.\n",
    "#### The result is an instance of the class W2VTransformer(size=100, min_count=1, seed=2)\n",
    "#### from gensim.sklearn_api import W2VTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open(\"./assets/text8_W2V.pickle\", \"rb\")\n",
    "text8_model = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of the task is to derive the model's understanding of how similar these terms are to the string \"party\". One way to do this:\n",
    "\n",
    "1. Use the word2vec `text8_model` to convert `my_ranking` into the corresponding set of word embedding vectors.\n",
    "2. Determine the cosine similarity of each word embedding relative to the target embedding of `\"party\"`. Maintain a list of tuples in the format of `(similarity_score, word)`. Do not skip over the embedding for the target word `\"party\"`, the list should include the similarity to itself.\n",
    "3. Sort the list in decreasing order. Remember, when sorting elements of type tuple, the first element determines the sorting order and the second element is only considered if there is a tie.\n",
    "4. Create a list variable called `system_ranking` which contains only the second element of each tuple (the words). \n",
    "\n",
    "Your function should return a tuple in the format of `(my_ranking, system_ranking)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b04371ecb1a07994d50f78f9d32478f4",
     "grade": false,
     "grade_id": "cell-0c0ab801540e5261",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_word2vec1():\n",
    "    \n",
    "    embeddings = text8_model.transform(my_ranking)\n",
    "\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(9):\n",
    "        similarity = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[i].reshape(1, -1)).item()\n",
    "        temp_tuple = (similarity, my_ranking[i])\n",
    "        score_list.append(temp_tuple)\n",
    "\n",
    "    sorted_scores = sorted(score_list, key = lambda x: x[0], reverse = True)\n",
    "    system_ranking = [x[1] for x in sorted_scores]\n",
    "    result = my_ranking, tuple(system_ranking)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb810090fedc1324262e30c7e0a3aa2a",
     "grade": true,
     "grade_id": "cell-5f0e92e10d2835d3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reference_terms = (\n",
    "    \"party\",\n",
    "    \"bicycle\",\n",
    "    \"vote\",\n",
    "    \"lead\",\n",
    "    \"election\",\n",
    "    \"champagne\",\n",
    "    \"event\",\n",
    "    \"fun\",\n",
    "    \"budget\",\n",
    ")\n",
    "\n",
    "stu_ans = answer_word2vec1()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q3.1: Your function should return a tuple. \"\n",
    "assert (\n",
    "    len(stu_ans) == 2\n",
    "), \"Q3.1: Your tuple should contain two elements: a tuple of strings (my_ranking), and a tuple of strings (system_ranking).\"\n",
    "\n",
    "# check my_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[0], tuple\n",
    "), \"Q3.1: Your first element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[0]) == len(\n",
    "    reference_terms\n",
    "), \"Q3.1: Your my_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Q3.1: Your my_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[0]) == set(\n",
    "    reference_terms\n",
    "), \"Q3.1: Your my_rankings tuple is not a permutation of the permitted terms.\"  # must be a permutation of the official term set\n",
    "\n",
    "# check system_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[1], tuple\n",
    "), \"Q3.1: Your second element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[1]) == len(\n",
    "    reference_terms\n",
    "), \"Q3.1: Your system_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Q3.1: Your system_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[1]) == set(\n",
    "    reference_terms\n",
    "), \"Q3.1: Your system_rankings tuple is not a permutation of the permitted terms.\"  # must be a permutation of the official term set\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 Comparing rankings with Spearman correlation coefficient\n",
    "\n",
    "Given the rankings you generated above in the format of `(my_ranking, system_ranking)`, use the [`scipy.stats.spearmanr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) function which will return a SciPy object containing the spearman correlation and the p-value of your ranking compared to the system ranking, as a measurement of how well they agree. Convert this spearman result to a tuple, and append that tuple to your previous results, i.e. it should have a format of `(my_ranking, system_ranking, spearman_tuple)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19533debbb8e8c5e54f54879cd812cc0",
     "grade": false,
     "grade_id": "cell-d3cd084f9be90457",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def answer_word2vec():\n",
    "\n",
    "    my_ranking, system_ranking = answer_word2vec1()\n",
    "    result = my_ranking, system_ranking, tuple(spearmanr(my_ranking, system_ranking))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee6529a38ad3d0116357edaf013bca87",
     "grade": true,
     "grade_id": "cell-b53ed7c82b9dceeb",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reference_terms = (\n",
    "    \"party\",\n",
    "    \"bicycle\",\n",
    "    \"vote\",\n",
    "    \"lead\",\n",
    "    \"election\",\n",
    "    \"champagne\",\n",
    "    \"event\",\n",
    "    \"fun\",\n",
    "    \"budget\",\n",
    ")\n",
    "\n",
    "stu_ans = answer_word2vec()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q3.2: Your function should return a tuple. \"\n",
    "assert (\n",
    "    len(stu_ans) == 3\n",
    "), \"Q3.2: Your tuple should contain three elements: a tuple of strings (my_ranking), a tuple of strings (system_ranking), a tuple with spearman output (2 floats).\"\n",
    "\n",
    "# check my_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[0], tuple\n",
    "), \"Q3.2: Your first element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[0]) == len(\n",
    "    reference_terms\n",
    "), \"Q3.2: Your my_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Q3.2: Your my_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[0]) == set(\n",
    "    reference_terms\n",
    "), \"Q3.2: Your my_rankings tuple is not a permutation of the permitted terms.\"  # must be a permutation of the official term set\n",
    "\n",
    "# check system_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[1], tuple\n",
    "), \"Q3.2: Your second element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[1]) == len(\n",
    "    reference_terms\n",
    "), \"Q3.2: Your system_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Q3.2: Your system_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[1]) == set(\n",
    "    reference_terms\n",
    "), \"Q3.2: Your system_rankings tuple is not a permutation of the permitted terms.\"  # must be a permutation of the official term set\n",
    "\n",
    "# check spearmanr\n",
    "assert isinstance(\n",
    "    stu_ans[2], tuple\n",
    "), \"Q3.2: Your third element must be a tuple (of two floats). \"\n",
    "assert (\n",
    "    len(stu_ans[2]) == 2\n",
    "), \"Q3.2: Your spearman output tuple should contain two floats.\"\n",
    "assert isinstance(\n",
    "    stu_ans[2][0], (float, np.floating)\n",
    "), \"Q3.2: Your spearman corr should be a float. \"\n",
    "assert isinstance(\n",
    "    stu_ans[2][1], (float, np.floating)\n",
    "), \"Q3.2: Your spearman p-val should be a float. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86e4cd96bee3a57161d3261bd93d40c5",
     "grade": false,
     "grade_id": "cell-710cb95aafc08d00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4: (30 points) Topic coherence.\n",
    "\n",
    "One measure of topic model quality that is used e.g. to determine the optimal number of topics for a corpus is *topic coherence*. This is a measure of how semantically related the top terms in a topic model are. Topic models with low coherence tend to be filled with seemingly random words and hard to interpret, while high coherence usually indicates a clear semantic theme that's easily understood.\n",
    "\n",
    "With their ability to represent word semantics, word embeddings are an ideal tool for computing topic coherence. In part 1, you'll implement a simple topic coherence function. In part 2, you'll apply that function to NMF topic modeling to find a setting for the number of topics that gives maximally coherent topic models.\n",
    "\n",
    "We're going to use the same `text8_model` W2VTransformer object, which implements the word2vec embedding, that you loaded for the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "824f1fd3ac9ea8cda61e5d15fc0156cf",
     "grade": false,
     "grade_id": "cell-94d0a243f11950b9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Part 4.1. (15 points) Average semantic distance as a text coherence measure.\n",
    "Implement a function that takes a list of terms (strings) as input and returns a positive float indicating their semantic coherence. Here is the algorithm you should use:\n",
    "\n",
    "1. For each input term, compute its word2vec embedding vector. One problem you might encounter is that some terms may not exist in the word2vec model. You get a \"KeyError\" exception when trying to transform that \"out-of-vocabulary\" term. You should ignore these terms: one way to do this is by wrapping your embedding call with a try/except statement that catches the KeyError and just ignores that word, and continues processing.\n",
    "\n",
    "2. Once you have the list of embedding vectors for the input terms, compute their pairwise cosine similarity. If there are $n$ embedding vectors, this step will result in an $n x n$ matrix D.  If for some reason there are no input terms remaining (they are all out-of-vocabulary) just return 0.\n",
    "\n",
    "3. Obviously the most similar word to a term is itself, indicated by a \"1\" on the diagonal of $D$. But we don't want those: we only care about the pairwise distances to *other* terms, so to deal that case, set the diagonal to zero.\n",
    "\n",
    "4. Return the mean over all pairwise distances in D (with self-distances set to zero).  This is our simple coherence measure.\n",
    "\n",
    "Be sure to try it out on some samples. For example, here's what our reference implementation returns:\n",
    "\n",
    "`topical_coherence(['car', 'airplane', 'taxi', 'bus', 'vehicle', 'transport'])`\n",
    "\n",
    "0.46063321000999874\n",
    "\n",
    "`topical_coherence(['apple', 'banana', 'cherry', 'watermelon', 'lemon', 'orange'])`\n",
    "\n",
    "0.43306025200419956\n",
    "\n",
    "`topical_coherence(['possible', 'mean', 'volcano', 'feature', 'record', 'quickly'])`\n",
    "\n",
    "0.1150558124192887\n",
    "\n",
    "Your function should return the above measure of topic coherence for the following three lists, as a tuple of three corresponding floats:\n",
    "\n",
    "`['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport']`\n",
    "\n",
    "`['scsi', 'drive', 'computer', 'storage', 'megabyte']`\n",
    "\n",
    "`['introduction', 'pickle', 'guard', 'red', 'valiant']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8789aa0ef28ae3dce04a9c35ead85271",
     "grade": false,
     "grade_id": "cell-c98ec559f8390f02",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_coherence_a():\n",
    "\n",
    "    input_strings = [['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport'], \n",
    "                     ['scsi', 'drive', 'computer', 'storage', 'megabyte'],\n",
    "                     ['introduction', 'pickle', 'guard', 'red', 'valiant']]\n",
    "\n",
    "    score_list = []\n",
    "\n",
    "    for h in range(len(input_strings)):\n",
    "        temp_list = []\n",
    "        for i in range(len(input_strings[h])):\n",
    "            try:\n",
    "                vec = text8_model.transform(input_strings[h][i])\n",
    "                temp_list.append(vec)\n",
    "            except:\n",
    "                print('not an available word')\n",
    "        \n",
    "        embeddings = np.vstack(temp_list)\n",
    "        embedding_rows, embedding_columns = embeddings.shape\n",
    "\n",
    "        similarity_array = np.zeros((embedding_rows, embedding_rows))\n",
    "        for i in range(embedding_rows):\n",
    "            for j in range(embedding_rows):\n",
    "                sim = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1)).item()\n",
    "                similarity_array[i, j] = sim\n",
    "        \n",
    "        np.fill_diagonal(similarity_array, 0)\n",
    "        final_score = np.mean(similarity_array)\n",
    "        score_list.append(final_score)\n",
    "        result = tuple(score_list)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33982cd0380ae075afea093d5c5bc7df",
     "grade": true,
     "grade_id": "cell-2c8db568c2a5c7b7",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_coherence_a()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q4.1: Your function should return a tuple. \"\n",
    "assert (\n",
    "    len(stu_ans) == 3\n",
    "), \"Q4.1: Your function should return a tuple of three elements. \"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Q4.1: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70b612235f727414c3895b892ab5eadf",
     "grade": false,
     "grade_id": "cell-218be1194a1f822d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Part 4.2 (15 points) Applying semantic coherence to topic model selection.\n",
    "\n",
    "Now you'll use the semantic coherence measure you developed in Part 1 with topic models computed using Non-Negative Matrix Factorization.\n",
    "\n",
    "Implement a simple loop that trains an NMF topic model, for number of topics **from 2 to 10 inclusive**. At each iteration, compute your topic coherence measure on the **top 10** words for each topic. Then compute the *median* topic coherence over all these topic scores.\n",
    "\n",
    "Your function should return a list of 9 median coherence scores, corresponding to each choice of the number of topics to use with NMF.  Which choice gives the highest median semantic coherence?\n",
    "\n",
    "When creating the NMF object, use these parameter settings: `random_state=42, init=\"nndsvd\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef87985e45110dd4f41e0bde67d0b2ce",
     "grade": false,
     "grade_id": "cell-2840e7647d40fa8f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Use the following code to prepare input to the NMF topic model.\n",
    "### It assumes you've loaded the 20newgroups variables at the beginning of this assignment\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_NMF = TfidfVectorizer(\n",
    "    max_features=20000,  # only top 5k by freq\n",
    "    lowercase=True,  # drop capitalization\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=2,  # note: absolute count of doc\n",
    "    max_df=0.05,  # note: % of docs\n",
    "    token_pattern=r\"\\b[a-z]{3,12}\\b\",  # remove short, non-word-like terms\n",
    "    stop_words=\"english\",\n",
    ")  # default English stopwords\n",
    "\n",
    "tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(documents_train)\n",
    "feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0ac1382a01695e2a022ba8006e1b389",
     "grade": false,
     "grade_id": "cell-cea38c8b4154639b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import statistics\n",
    "\n",
    "def answer_coherence_b():\n",
    "    \n",
    "    score_list = []\n",
    "\n",
    "    for i in range(2, 11):\n",
    "    \n",
    "        n_topics = i\n",
    "        X = tfidf_documents_NMF\n",
    "\n",
    "        nmf = NMF(n_components=n_topics, random_state=42, init=\"nndsvd\")\n",
    "        W = nmf.fit_transform(X) \n",
    "        H = nmf.components_\n",
    "\n",
    "        top = 10\n",
    "        topic_index_max = n_topics\n",
    "    \n",
    "        topic_list = []\n",
    "\n",
    "        for topic_index in range(0, topic_index_max):\n",
    "            top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "            top_terms = []\n",
    "        \n",
    "            for term_index in top_indices[0:top]:\n",
    "                top_terms.append(feature_names_NMF[term_index])\n",
    "        \n",
    "            topic_list.append(top_terms)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        input_strings = topic_list\n",
    "        input_means = []\n",
    "\n",
    "        for h in range(len(input_strings)):\n",
    "            temp_list = []\n",
    "            for i in range(len(input_strings[h])):\n",
    "                try:\n",
    "                    vec = text8_model.transform(input_strings[h][i])\n",
    "                    temp_list.append(vec)\n",
    "                except:\n",
    "                    print('not an available word')\n",
    "        \n",
    "            embeddings = np.vstack(temp_list)\n",
    "            embedding_rows, embedding_columns = embeddings.shape\n",
    "\n",
    "            similarity_array = np.zeros((embedding_rows, embedding_rows))\n",
    "            for i in range(embedding_rows):\n",
    "                for j in range(embedding_rows):\n",
    "                    sim = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1)).item()\n",
    "                    similarity_array[i, j] = sim\n",
    "        \n",
    "            np.fill_diagonal(similarity_array, 0)\n",
    "            mean_score = np.mean(similarity_array)\n",
    "            input_means.append(mean_score)\n",
    "            \n",
    "        median_score = statistics.median(input_means)\n",
    "        score_list.append(median_score)\n",
    "        result = score_list\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0da55eea5b08ae60f131402aa1b3477a",
     "grade": true,
     "grade_id": "cell-f6b832a23d7d50a8",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_coherence_b()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q4.2: Your function should return a list. \"\n",
    "assert (\n",
    "    len(stu_ans) == 9\n",
    "), \"Q4.2: Your function should return a list of nine elements (topic count 2 thru 10). \"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Q4.2: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_ans"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_unsupervised_learning_v1_assignment3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
